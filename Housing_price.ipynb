{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "541700f3e93a6002c36b86afd953bf589960418f"
      },
      "cell_type": "markdown",
      "source": "# Exploring the data\n##  Loading Data\nRead the Iowa data file into a Pandas DataFrame called `home_data`."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Path of the file to read\niowa_train_path = '../input/train.csv'\niowa_test_path = '../input/test.csv'\n\n# Read csv\nhome_data = pd.read_csv(iowa_train_path)\nhome_data_test = pd.read_csv(iowa_test_path)\n\n# Look up dataset\nhome_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "98bc227f0652dfd634cf2dd9db80dce8d5bad46f"
      },
      "cell_type": "markdown",
      "source": "##  Review The Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b1c2a9984ec7ab4d906951df386502332234e06"
      },
      "cell_type": "code",
      "source": "# Print summary statistics\nhome_data.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89e1507384f625e6dc9793bbbd3f4b9b9dc36f44"
      },
      "cell_type": "markdown",
      "source": "##  Assigning Prediction Target\nSelect the target variable, which corresponds to the sales price. Saving this to a new variable called `y`. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85f5b157bd359d0ef95365724289298977056581"
      },
      "cell_type": "code",
      "source": "# print the list of columns in the dataset\nhome_data.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f67301ec9db7931fd2e95d2714e5cb9e14ea8ac1"
      },
      "cell_type": "code",
      "source": "# predicted training set\ny = home_data.SalePrice",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a4e320aaeaa38d44e9a2d4003e807ac4836aece6"
      },
      "cell_type": "markdown",
      "source": "##  Create X\n create a DataFrame called `X` holding the predictive features.\n\nusing just the following columns in the list :\n    * LotArea\n    * YearBuilt\n    * 1stFlrSF\n    * 2ndFlrSF\n    * FullBath\n    * BedroomAbvGr\n    * TotRmsAbvGrd"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb63deb8acc1ec3197d746b182328a12f43e0988"
      },
      "cell_type": "code",
      "source": "# Create the list of limited features below\nfeature_names = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\n\n# select data corresponding to features in feature_names for training set\nX = home_data[feature_names]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f38509f7e00728751634d45654a6842b05d4a0a4"
      },
      "cell_type": "code",
      "source": "# print description or statistics from X\nprint(X.describe())\n\n# print the top few lines\nprint(X.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "683cb93b213808bd3502ff5cf520bb54a52cfaa9"
      },
      "cell_type": "markdown",
      "source": "# Machine Learning model\n## Specify and Fit Model\nCreate a `DecisionTreeRegressor` and save it iowa_model. \n\nfit the model just created using the data in `X` and `y` saved above."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e15c0f1d682153825d6a59e3f7e31c818ae31ee",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeRegressor\n#specify the model; model used here is DecisionTree. \niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit the model\niowa_model.fit(X,y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d9cea1f4441d056d7608787069a8a08a6211b87"
      },
      "cell_type": "code",
      "source": "predictions = iowa_model.predict(X)\nprint(predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3cbee85aad21a91b8350a93543edafb83cbb39cc"
      },
      "cell_type": "markdown",
      "source": "## Make Predictions\nMake predictions with the model's `predict` command using `X` as the data. Save the results to a variable called `predictions`."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5962d0a68cce47f8356232dacd0041abe6da237e"
      },
      "cell_type": "code",
      "source": "print(predictions[:5])\nprint(y[:5])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47a93472c542f00040ac3a4086c802cd1074f8b1"
      },
      "cell_type": "markdown",
      "source": "# Model Validation\n## Split Your Data\nUse the `train_test_split` function to split up your data.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31d0d93010a20b99ebe3b042f1b47a6432c53e84"
      },
      "cell_type": "code",
      "source": "# Import the train_test_split function\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "961706f02e04c13c5e77c164bbb6be3198fd357c"
      },
      "cell_type": "markdown",
      "source": "## Specify and Fit the Model\n\nCreate a `DecisionTreeRegressor` model and fit it to the training data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f715c6ac5455e53884b9d6c98cb01089a4c020b9"
      },
      "cell_type": "code",
      "source": "# Specify the model\niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit iowa_model with the training data.\niowa_model.fit(train_X,train_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e52c7670e7a4de166e30059c1c2fce9811f1fa1d"
      },
      "cell_type": "markdown",
      "source": "## Make Predictions with Validation data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61ef279e3bc4a06abd055082dd0ff225aa258fb0"
      },
      "cell_type": "code",
      "source": "val_predictions = iowa_model.predict(val_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "028e114fe6739d3cb0f86ec44c8fb2ef5311429f"
      },
      "cell_type": "code",
      "source": "# print the top few validation predictions\nprint(val_predictions[:5])\n# print the top few actual prices from validation data\nprint(y.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84f32d0d066449936119038bdf503676589afa8b"
      },
      "cell_type": "markdown",
      "source": "## Calculate the Mean Absolute Error in Validation Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5120069ebb68410247d1f12e70930efd7cd4d142"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_absolute_error\nval_mae = mean_absolute_error(val_y,val_predictions)\n\n# uncomment following line to see the validation_mae\nprint(val_mae)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec3f9a24f821f0d7711f57a4c20c0771c9c9e73d"
      },
      "cell_type": "markdown",
      "source": "# Preventing underfitting and overfitting to find optimal solution\n## Function to return the mean absolute error for each combination of leaf nodes of a decision tree"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ce53fc1b1535ff09006d66cfa08e5ffc807aa26"
      },
      "cell_type": "code",
      "source": "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0380f70bba135df14ba87098341d07fcc605233b"
      },
      "cell_type": "markdown",
      "source": "## Compare Different Tree Sizes\na loop that tries the following values for *max_leaf_nodes* from a set of possible values.\n\nCall the *get_mae* function on each value of max_leaf_nodes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72f98a8b962d3333a015e629046befb7788a25c6"
      },
      "cell_type": "code",
      "source": "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor leaf in candidate_max_leaf_nodes:\n    my_mae = get_mae(leaf,train_X, val_X, train_y, val_y)\n    print(\"leaf:\",leaf,\"MAE:\",my_mae)\n# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\nbest_tree_size = 100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91c0ac548b04172b66502e9ec84e1b029f6edaf5"
      },
      "cell_type": "markdown",
      "source": "## Fit Model Using All Data\nit would be even more accurate by using all of the data and keeping that tree size which is best"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b61004ff7f07ff37c13609956b1bb6c015dec60a"
      },
      "cell_type": "code",
      "source": "# Fit the model with best_tree_size.\nfinal_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n\n# fit the final model\nfinal_model.fit(X, y)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ef685bbda29fffc788f66237089c631030dc977b"
      },
      "cell_type": "markdown",
      "source": "## Use a Random Forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65737eb62f901c21a6da778291cc39b9782b667a"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\n\n# Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(random_state=1)\n\n# fit your model\nrf_model.fit(train_X,train_y)\n\n# Calculate the mean absolute error of your Random Forest model on the validation data\nrf_val_mae = mean_absolute_error(rf_model.predict(val_X),val_y)\n\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc755c12831ede13cd0242f2b700cf1e286027bf"
      },
      "cell_type": "markdown",
      "source": "# Handling missing values\n## Analysing missing values in entire dataset (Testing purpose)\n## Trial and error of learnt concepts starts here"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddff76dc8124e6f4a19872a96b166ce7e5bfe531"
      },
      "cell_type": "code",
      "source": "# X.replace('NA', np.NaN)\n\nmissing_val_count_by_column = (home_data.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "db332e3e9cc6b2c6f605ff2aab3f01e15731f600"
      },
      "cell_type": "markdown",
      "source": "### On analysing the above missing values, out of a dataset of 1461 training values, 'Alley','PoolQC','Fence' and 'MiscFeature' have many null values\n### and hence can be dropped"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06db4d90f73848b010800cceeb4418e03ec5a4b6"
      },
      "cell_type": "code",
      "source": "# creating a copy of original dataset\nhome_data_cpy = home_data.copy()\n\n#calculating threshold and setting it to 80%\nthresh = len(home_data_cpy) * .2\n\n#removing column if contains 80% of values as NA\nhome_data_cpy.dropna(thresh = thresh, axis = 1, inplace = True)\n\nmissing_val_count_by_column = (home_data_cpy.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3685772972c68083fb887bfce0838dce46a36ae"
      },
      "cell_type": "markdown",
      "source": "## Using imputer to fill the null values"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "20904f46defe131e60abe96b06d29500ff4a3fcc"
      },
      "cell_type": "code",
      "source": "# columns containing null values\nnull_col = [col for col in home_data_cpy\n                   if home_data_cpy[col].isnull().any()]\nprint(\"Null values before imputation:\",null_col)\nnull_float_data = home_data_cpy.loc[:,home_data_cpy.dtypes==float]\n\n#import imputer class\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimp_col = my_imputer.fit_transform(null_float_data)\n\nimp_col",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d20943189582f0a1407020f535ad75614d0cd320"
      },
      "cell_type": "markdown",
      "source": "## Trial and error of concepts ends here"
    },
    {
      "metadata": {
        "_uuid": "d0f82380ff01b921cc4e27ba37cc5c39100b6b4b"
      },
      "cell_type": "markdown",
      "source": "## Differentiating categorical and numerical variables to perform One hot encoding on categorical variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68928b77cc806d7916c0b46f2b16084c9bdaf9a3"
      },
      "cell_type": "code",
      "source": "train_pred = home_data_cpy.drop(['Id','SalePrice'],axis=1)\n\n# low cardinality columns are those having less unique values\nlow_cardinality_col = [cname for cname in train_pred if train_pred[cname].nunique()<10 and train_pred[cname].dtype == \"object\"]\nlow_cardinality_col",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd96e22b76b8405c50fa716a8821c390a52bdf05"
      },
      "cell_type": "markdown",
      "source": "## Determining the numerical columns\n### One hot encoding can only be performed for categorical variables\n### One hot encoding is performed so that the entire dataset is of same datatype, for the machine learning models to predict and also for performing imputation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e31bd2a48fd6f03a87991bb646b0e4806ba746a"
      },
      "cell_type": "code",
      "source": "numerical_col = [cname for cname in train_pred if train_pred[cname].dtype in ['int64','float64']]\nnumerical_col",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97bf33fb4d150dedb53bf3513006ade5c328f502"
      },
      "cell_type": "markdown",
      "source": "## Using One hot encoder to encode categorical variables"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "cc55ed1dee02e15d716936496b2f624f8853e460"
      },
      "cell_type": "code",
      "source": "# from sklearn.preprocessing import OneHotEncoder\n# onehotencoder = OneHotEncoder(categorical_features = low_cardinality_col)\n# train_X_sample = onehotencoder.fit_transform()\ntrain_pred = pd.get_dummies(train_pred)\nlow_cardinality_col = [cname for cname in train_pred if train_pred[cname].nunique()<10 and train_pred[cname].dtype == \"object\"]\nlow_cardinality_col\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00382537ca909d05d6bf199de90c1ca2f27e0ca6"
      },
      "cell_type": "code",
      "source": "missing_val_count_by_column = (train_pred.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aa238da4790ad7cce6e8a3221dedace5f4a6d52e"
      },
      "cell_type": "markdown",
      "source": "## Previously inputation was not able to be performed due to the presence of categorical variables, now since they have been encoded, the null values can be imputed using Imputer"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "86ef3184ffd7816d462d2800d3a3e43eda77701d"
      },
      "cell_type": "code",
      "source": "my_imputer = SimpleImputer()\nimp_col = my_imputer.fit_transform(train_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e9d95ccee1e26e68ff3a88e602a23a3e6cbed19b"
      },
      "cell_type": "markdown",
      "source": "## End of another trial and error of One hot encoding"
    },
    {
      "metadata": {
        "_uuid": "a54fc319cc9707a6116bc93b5ff00c3c92d10e61"
      },
      "cell_type": "markdown",
      "source": "# XGBoost Model\n## Fitting XGBooset model to training set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c465428379f1ed0c5bbbc8df1168270fe875f38d"
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_X, train_y, verbose=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76740e772b571a5697e98b5a7e3eebfce7f66688"
      },
      "cell_type": "code",
      "source": "# make predictions\npredictions = my_model.predict(val_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, val_y)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8dea3824c1cb71be155a5c161dbac2fa5080392a"
      },
      "cell_type": "code",
      "source": "my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(val_X, val_y)], verbose=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "418908d3ed5a9e4e5a357a5db91981f257d9a1cb"
      },
      "cell_type": "code",
      "source": "# make predictions\npredictions = my_model.predict(val_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, val_y)))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}